---
title: "Homework 3"
subtitle: "STAT 471"
author:
- Jacob Kahn
- Devesh Dayal
- Meghana Jayam
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Preflight Tasks

We have a lot of existing variables, so we'll clean them out.
```{r}
rm(list=ls())
```
Check our working directory. This changed for various group members, so we each set it locally:
```{r}
# setwd(dir)
# getwd() # check that working directory
```

# Problem 1
## Part a
Generate a predictor X of length n=100, as well as a noise vector epsilon of length n=100.
```{r}
set.seed(10)
X <- rnorm(100)
epsilon <- rnorm(100)
```

## Part b
Generate a response vector Y of length n=100 with B0, B1, B2, B3 
```{r}
B0 <- -2
B1 <- 0.1
B2 <- 1
B3 <- 5
Y <- B0+B1*X+B2*X^2+B3*X^3+epsilon
```

## Part c
Perform best subset selection (find Cp, BIC, adjr2)
```{r}
library(leaps) # for regsubsets
data <- data.frame(y=Y, x=X)
regsub <- regsubsets(y~poly(x,degree=10,raw=TRUE), data=data, nvmax=10)
reg.sum <- summary(regsub)
names(reg.sum)

# find optimal size by getting min Cp, BIC, adjr2
which.min(reg.sum$cp)       # 2
which.min(reg.sum$bic)      # 2
which.max(reg.sum$adjr2)    # 2

# plot Cp, BIC, adjr2
plot(reg.sum$cp, xlab="Number of predictors", ylab="Cp", col="red", type="p", pch=16) # exhibit 1
plot(reg.sum$bic, xlab="Number of predictors", ylab="BIC", col="red", type="p", pch=16) # exhibit 2
plot(reg.sum$adjr2, xlab="Number of predictors", ylab="adjr2", col="red", type="p", pch=16) # exhibit 3
```
As seen by the plots, we would use a 2-variable model with Cp, a 2-variable model with BIC, and a 2-variable model with Adjusted R^2.
```{r}
coef(regsub, id=2)
```
The model will use x^2 and x^3.

## Part d
Use forward selection and the backward selection to compare esults with part c.
First, we will do forward selection:
```{r}
regsub.f <- regsubsets(y~poly(x,degree=10,raw=TRUE), data=data, nvmax=10, method="forward")
reg.sum.f <- summary(regsub.f)

which.min(reg.sum.f$cp)       # 2
which.min(reg.sum.f$bic)      # 2
which.max(reg.sum.f$adjr2)    # 2

plot(reg.sum.f$cp, xlab="Number of predictors", ylab="Cp", col="red", type="p", pch=16) # exhibit 4
plot(reg.sum.f$bic, xlab="Number of predictors", ylab="BIC", col="red", type="p", pch=16) # exhibit 5
plot(reg.sum.f$adjr2, xlab="Number of predictors", ylab="adjr2", col="red", type="p", pch=16) # exhibit 6

coef(regsub.f, id=2)
```
For forward selection, we see the same reults as in part c.
Now, we will do backward selection:
```{r}
regsub.b <- regsubsets(y~poly(x,degree=10,raw=TRUE), data=data, nvmax=10, method="backward")
reg.sum.b <- summary(regsub.b)

which.min(reg.sum.b$cp)       # 2
which.min(reg.sum.b$bic)      # 2
which.max(reg.sum.b$adjr2)    # 2

plot(reg.sum.b$cp, xlab="Number of predictors", ylab="Cp", col="red", type="p", pch=16) # exhibit 7
plot(reg.sum.f$bic, xlab="Number of predictors", ylab="BIC", col="red", type="p", pch=16) # exhibit 8
plot(reg.sum.f$adjr2, xlab="Number of predictors", ylab="adjr2", col="red", type="p", pch=16) # exhibit 9

coef(regsub.b, id=2)
```
Again, we see the same model and values.

## Part e
Fit a LASSO model and use cross-validation to select the optimal value for lambda.
```{r}
library(glmnet) # for LASSO
X.lasso <- model.matrix(y~poly(x,degree=10,raw=TRUE), data=data)[,-1]
Y.lasso <- Y
lasso.cv <- cv.glmnet(X.lasso, Y.lasso, alpha=1, nfolds=10)
plot(lasso.cv) # exhibit 10

plot(lasso.cv$lambda) # exhibit 11
lambda.final <- lasso.cv$lambda.min # 0.05631109

beta <- coef(lasso.cv, s=lambda.final)
beta <- as.matrix(beta)
beta
```
Using the LASSO method, the model picks x, x^2, x^3, x^4, x^5, x^9. The coefficients fo x^9, x, and x^5 are more negligible than the others.

## Part f
```{r}
B7 <- 7
Y.f <- B0 + B7*X^7 + epsilon
data.f <- data.frame(y=Y.f, x=X)
```
Model selection using regsubsets:
```{r}
regsub.partf <- regsubsets(y~poly(x,degree=10,raw=TRUE), data=data.f, nvmax=10)
reg.sum.partf <- summary(regsub.partf)

which.min(reg.sum.partf$cp)       # 1
which.min(reg.sum.partf$bic)      # 1
which.max(reg.sum.partf$adjr2)    # 1

coef(regsub.partf, id=1)
```
All three methods chose one-variable models.
Now, using LASSO:
```{r}
X.lasso.f <- model.matrix(y~poly(x,degree=10,raw=TRUE), data=data.f)[,-1]
Y.lasso.f <- Y.f
lasso.cv.f <- cv.glmnet(X.lasso.f, Y.lasso.f, alpha=1, nfolds=10)
plot(lasso.cv.f) # exhibit 12

lambda.final.f <- lasso.cv.f$lambda.min # 10.43878

beta <- coef(lasso.cv.f, s=lambda.final.f)
beta <- as.matrix(beta)
beta
```
Using LASSO, the model selected is also a one-variable model.

# Problem 2

## Part 1 - Data Summary & Inspection

```{r, , message=F, warning=F}
# Library imports
library(dplyr)
library(ggplot2)
library(mapproj)
library(viridis)
# Import the crime data set
crime.data <- read.csv("CrimeData.csv", header=T, na.string=c("", "?"))
dim(crime.data)
```

We use the example code (provided below) from the file "Rcode_CrimeRate_Summary_dplyr_heatmap.r", for preprocessing data and drawing heatmaps.

```{r}
preprocessing <- function (data) {
  # Preprocess the data for plotting the heatmap 
  #
  # Args:
  #   data: data with two columns including the abbre of states 
  #         along with the corresponding target number
  #
  # Returns:
  #   Add standard state name with state coordination
  
  # standard state name to match with mapdata
  data$region <- tolower(state.name[match(data$state,state.abb)])
  
  # state coordination, i.e. latitutde and longitude
  data$center_lat  <- state.center$x[match(data$state, state.abb)]
  data$center_long <- state.center$y[match(data$state, state.abb)]

  data
}

plot.heatmap <- function(data, mapdata, target) {
  # Plot out the heatmap 
  #
  # Args:
  #   data: data of standard state name and coordination with corresponding 
  #         target data
  #   mapdata: mapdata following the format of map_data in ggplot
  #   target: name of the target
  #
  # Returns:
  #   The heatmap of the target
  
  data <- preprocessing(data)
  
  # merge the data with the map
  map <- merge(mapdata, data, sort=FALSE, by="region", all.x=TRUE)
  map <- map[order(map$order),]
  
  # calculate the target range
  min <- eval(parse(text=paste("min(data$", target,")")))
  min_digits <- unlist(strsplit(as.character(floor(min)), ""))
  min_range <- as.numeric(min_digits[1]) * 10^(length(min_digits)-1)

  max <- eval(parse(text=paste("max(data$", target, ")")))
  max_range <- round(max, -floor(log10(max)))
   
  # plot the map 
  heapmap <- ggplot(map, aes(x=long, y=lat, group=group))
  heapmap <- heapmap + eval(parse(text=paste("geom_polygon(aes(fill=", target,"))")))
  heapmap <- heapmap + geom_path()
  heapmap <- heapmap + geom_text(data=data, aes(x=center_lat, y=center_long, group=NA, label=state, size=2))
  legend_name <- target
  heapmap <- heapmap + scale_fill_continuous(limits=c(min_range, max_range), name=target)
  # you can specify the color by the Hex Color Code
  # heapmap <- heapmap + scale_fill_gradient(low="#0099CC", high="#003366")
  heapmap <- heapmap + scale_fill_viridis()
  # heapmap <- heapmap + scale_fill_viridis(option="magma")
  heapmap
}
```

We can now process the input data and plot a heatmap for our two selected variables of interest: mean percentage unemployed and mean percentage of the population under poverty.
We create a dataframe to encapsulate these variables from the original data source and use aggregated data to create heatmaps. We also retain in a summarized crime rate field, as calculated in the example code, for later reference.

```{r}
data.s <- summarize(group_by(crime.data, state), 
            mean.pct.unemployed=mean(pct.unemployed), 
            mean.pct.pop.underpov=mean(pct.pop.underpov),
            crime.rate=mean(violentcrimes.perpop, na.rm=TRUE), #ignore the missing values
            n=n())
# mapdata
states <- map_data("state") 

# Plot of crime rate
crime.rate <- data.s[, c("state", "crime.rate")]
plot.heatmap(crime.rate, states, "crime.rate")
```

With this information in mind, we can now plot our two variables of interest.

```{r, warning=F}
# warnings suppressed
# Plotting mean percentage unemployed
pct.unemployed <- data.s[, c("state", "mean.pct.unemployed")]
pct.unemployed

plot.heatmap(pct.unemployed, states, "mean.pct.unemployed")

# Plotting mean population under poverty
pct.pop.underpov <- data.s[, c("state", "mean.pct.pop.underpov")]
pct.pop.underpov

plot.heatmap(pct.pop.underpov, states, "mean.pct.pop.underpov")

```


### Observations
Mean Percentage Unemployed: We can see a relatively strong relationship between crime rate and mean percent unemployed based on the data in the heatmap. However, this may not be an accurate isolated predictor of crime rate, given the large variety of factors that can affect it in the real world. There are states like South Carolina and Minnesota for instance where we can see that the values for percent unemployed and crime rate are not correlated.

Mean Percentage Under Poverty: This heatmap is very strongly correlated to the values displayed for crime rate, as would be expected - areas under the natural rate of poverty in the country are prone to higher crime rates. As with unemployment however, there are exceptions to this matching, like the state of Idaho which has a high percent of population under poverty, but a relatively low crime rate.


## Part 2 - Analysis with LASSO and Elasticnet

Before we proceed with the analysis, as done in lecture, we remove redudant variables from the data set (which can be calculated later if needed). We finally combine clean data for only the states of FL and CA.

```{r}
# remove variables about police departments because of large number of missing values
data1 <- crime.data[,c(2,6:103,121,122,123, 130:147)]

# remove redudant variables (only need one of num.X and pct.X for instance)
var_names_out <- c("num.urban","other.percap", "num.underpov",
                   "num.vacant.house","num.murders","num.rapes",
                   "num.robberies", "num.assaults", "num.burglaries",
                   "num.larcenies", "num.autothefts", "num.arsons")

data1 <- data1[!(names(data1) %in% var_names_out)] # take some redundant var's out

# remove variables that can be recalculated when needed
names_other_crimes <- c( "murder.perpop", "rapes.perpop",                   
                        "robberies.perpop",  "assaults.perpop",                
                        "burglaries.perpop", "larcenies.perpop",               
                        "autothefts.perpop", "arsons.perpop",                  
                         "nonviolentcrimes.perpop")
# Take other crimes out
data2 <- data1[!(names(data1) %in% names_other_crimes)] # take other crimes out. 


# Lastly, we combine information for the states of FL and CA only
data.fl <- data2[data2$state=="FL",-1] # take state column out
data.ca <- data2[data2$state=="CA",-1]

# Combined data ready!
comb.data <- rbind(data.fl, data.ca)
```

Now we can proceed to generate a LASSO model.

```{r}
comb.data <- comb.data[complete.cases(comb.data),]

# X variables as a categorical matrix
X2 <- model.matrix(violentcrimes.perpop~., comb.data)[,-1]  

# response variable
Y2 <- comb.data$violentcrimes.perpop

# alpha set close to 1, with 10 folds of cross-validation
fit2.cv <- cv.glmnet(X2, Y2, alpha=1, nfolds=10) 
plot(fit2.cv)

# extract betas coefficients values for lambda.1se (fewer number of variables in the model for simplicity)
coef.1se <- coef(fit2.cv, s="lambda.1se")  
coef.1se <- coef.1se[which(coef.1se !=0),] 
coef.1se
rownames(as.matrix(coef.1se))

# use lambda values from above to
fit2.lasso <- glmnet(X2, Y2, alpha=1, lambda=fit2.cv$lambda.1se)
variables.final <- coef(fit2.lasso)
variables.final <- variables.final[which(variables.final !=0),]
variables.final <- rownames(as.matrix(variables.final))
variables.final

# join together the extracted betas for the final model
fit_formula = as.formula(paste("violentcrimes.perpop", "~", paste(variables.final[-1], collapse = "+")))
fit.final = lm(fit_formula, data=comb.data)
summary(fit.final) 
```


The predicted formula generated through the LASSO model (reading from the summary provided above) is violentcrimes.perpop = 2012.95 + 12.96(race.pctblack) - 22.68(pct.kids2parents) + 94.95(pct.kids.nvrmarried). 

### Cross validating lambdas and alphas


We now loop through 20 different values of alpha and lambdas and compare CVMs, choosing only the final model with the lowest CVM.

```{r}
Xb <- model.matrix(violentcrimes.perpop~., comb.data)[,-1]
Yb <- comb.data[, 98]

# test alpha values
alpha <- seq(0, 1, .05)
results <- data.frame()

for(i in 1:21){
  # 10 folds of CV each iteration
 temp_model<- cv.glmnet(Xb, Yb, alpha = alpha[i], family="gaussian", nfolds = 10, type.measure = "deviance") 
 pos<-which(temp_model$lambda==temp_model$lambda.1se)
 # store alpha, CVM pairs for easy reporting
 results[i,1] <- alpha[i]
 results[i,2] <- temp_model$cvm[pos]
}

pos.cvm<-which.min(results[,2])
cvm_final<-results[pos.cvm,2]
cvm_final
alpha_final<-results[pos.cvm,1]
alpha_final
```


We can see that the best model results (averaged over a series of iterations) with an alpha value of 0.85 (and corresponding CVM of 158220).

Below are our outputs based on the model we found above by crossvalidating the alphas and lambdas
```{r}
fit_alpha_final <- cv.glmnet(Xb, Yb, alpha = alpha_final, family="gaussian", nfolds = 10, type.measure = "deviance") 
fit_alpha_final$lambda.1se
coef_alpha_final.1se <- coef(fit_alpha_final, s="lambda.1se") 
coef_alpha_final.1se <- coef_alpha_final.1se[which(coef_alpha_final.1se !=0),] 
coef_alpha_final.1se
rownames(as.matrix(coef_alpha_final.1se))
```

Below we create an OLS model using the variables from the model determined through LASSO.
```{r}
fitQ2.lm<-lm(fit_formula, data=comb.data)
summary(fitQ2.lm)
```
Finally, we can see that the results in both types of models are significant at the 0.001 level, and that the OLS model is remarkably similar to the crossvalidated model generated with the best fit version of alpha.




